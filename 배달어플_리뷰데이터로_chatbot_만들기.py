# -*- coding: utf-8 -*-
"""배달어플 리뷰데이터로 Chatbot 만들기.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GbxwsIxubMgEX9qW6pTCzg4Pb0ZVv7Ib

#### 불용어 사전 : https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a

# LLM 만들기

### 1. 데이터 불러오기
"""

pip install google_play_scraper

### 배달의 민족 + 요기요 + 쿠팡이츠 데이터를 합친 3만건 데이터로 모델돌리기
### 모델의 범용성(일반화성능)증가, 편향감소, 텍스트의 다양성 증가
## 부정리뷰를 긍정리뷰로 잘못예측함 -> 라벨수정하기, 활성화함수인 시그모이드 함수사용, class_weight로 손실함수 가중치부여 , 학습데이터증강기법 사용하기

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

# 배달어플 순위
# 1. 배달의민족
# 2. 쿠팡이츠
# 3. 요기요

url = "com.sampleapp"
# com.coupang.mobile.eats-> 쿠팡이츠
# com.fineapp.yogiyo --> 요기요
# com.sampleapp --> 배달의민족

# 앱 정보 크롤링
app = app(url,lang = "ko", #언어 한국어
                country = "kr") #나라 한국으로 설정

# 앱 리뷰 크롤링
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5초 대기시간
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , 관련성 높은순으로 정렬
                    filter_score_with = None # None means All score
                    )
# 데이터프레임 변환
df_bm= pd.DataFrame(review)
df_bm.head(2) # 138278

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

# 배달어플 순위
# 1. 배달의민족
# 2. 쿠팡이츠
# 3. 요기요

url = "com.coupang.mobile.eats"
# com.coupang.mobile.eats-> 쿠팡이츠
# com.fineapp.yogiyo --> 요기요
# com.sampleapp --> 배달의민족

# 앱 정보 크롤링
app = app(url,lang = "ko", #언어 한국어
                country = "kr") #나라 한국으로 설정

# 앱 리뷰 크롤링
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5초 대기시간
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , 관련성 높은순으로 정렬
                    filter_score_with = None # None means All score
                    )
# 데이터프레임 변환
df_cp= pd.DataFrame(review)
df_cp.head(2) # 138278

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

# 배달어플 순위
# 1. 배달의민족
# 2. 쿠팡이츠
# 3. 요기요

url = "com.fineapp.yogiyo"
# com.coupang.mobile.eats-> 쿠팡이츠
# com.fineapp.yogiyo --> 요기요
# com.sampleapp --> 배달의민족

# 앱 정보 크롤링
app = app(url,lang = "ko", #언어 한국어
                country = "kr") #나라 한국으로 설정

# 앱 리뷰 크롤링
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5초 대기시간
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , 관련성 높은순으로 정렬
                    filter_score_with = None # None means All score
                    )
# 데이터프레임 변환
df_yo= pd.DataFrame(review)
df_yo.head(2) # 138278

"""### 2. 전처리- 배달의민족"""

best_bm= df_bm[['content','score']][df_bm['score']==5] # score 값이 1점인 리뷰만 추출
best_bm['score']=1 # 1로 라벨링
best_bm.drop_duplicates(subset=['content'], inplace=True)
best_bm = best_bm.dropna()
best_bm= best_bm[:5000]
best_bm

### 리뷰1점과 2점을 0으로 라벨링 해보기
worst_all_bm= df_bm[['content','score']] [(df_bm['score']==1) | (df_bm['score']==2)]
worst_all_bm['score']=0 # 0으로 라벨링
worst_all_bm.drop_duplicates(subset=['content'], inplace=True)
worst_all_bm = worst_all_bm.dropna()
worst_all_bm= worst_all_bm[:5000]
worst_all_bm

# # 결측치 및 중복값 제거
# best.drop_duplicates(subset=['content'], inplace=True)
# worst_all.drop_duplicates(subset=['content'], inplace=True)

# best = best.dropna()
# print(len(best)) # 67949

# worst_all = worst_all.dropna()
# print(len(worst_all)) # 18663

### 특수문자 제거 함수정의
import re
def clean_review_text(text):
    # 입력 텍스트가 None인 경우 처리
    if text is None:
        return ""

    # 4. 허용할 문자 정의
    # 한글, 영어 대소문자, 숫자, 그리고 감성 표현에 중요한 구두점(.,?!), 공백만 허용합니다.
    # [^ ] 안에 정의된 문자열 외의 모든 문자를 공백으로 대체합니다.
    text = re.sub(r'[^가-힣a-zA-Z0-9\s.,?!]', ' ', text)

    # 5. 연속된 공백을 하나의 공백으로 치환하고 양쪽 공백 제거
    text = re.sub(r'\s+', ' ', text).strip()

    return text

## 특수문자 제거 함수활용
# 특수문자 제거함으로써 모델 입력 데이터의 품질을 높이기
best_bm['content'] = best_bm['content'].apply(clean_review_text)

# 2. 부정 리뷰 데이터프레임 정제
worst_all_bm['content'] = worst_all_bm['content'].apply(clean_review_text)

# 3. 라벨 (score) 재설정 (만약 평점 외의 다른 라벨링이 필요하다면)
# 긍정 라벨은 1, 부정 라벨은 0으로 통일 (이진 분류 기준)
best_bm['label'] = 1
worst_all_bm['label'] = 0

best_bm = best_bm.drop(columns=['score'])
worst_all_bm = worst_all_bm.drop(columns=['score'])

# 데이터 정규화
best_bm['content'] = best_bm['content'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
worst_all_bm['content'] = worst_all_bm['content'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")

# 데이터 묶기
df_bm= pd.concat([best_bm,worst_all_bm])
df_bm.columns=['content','label']
df_bm

df_bm.isnull().sum() # 결측치 확인

# 각 라벨값이 유일한지 확인
print(df_bm['content'].nunique(), df_bm['label'].nunique()) # 천건이 중복됨
df_bm= df_bm.drop_duplicates(subset=['content'])

"""### 2-2. 전처리 - 함수로 묶기"""

### 데이터 전처리 함수
def Preprocessing(data):
  best= data[['content','score']][data['score']==5] # score 값이 1점인 리뷰만 추출
  best['score']=1 # 1로 라벨링
  best.drop_duplicates(subset=['content'], inplace=True)
  best = best.dropna()
  best= best[:5000]

  ### 리뷰1점과 2점을 0으로 라벨링 해보기
  worst_all= data[['content','score']] [(data['score']==1) | (data['score']==2)]
  worst_all['score']=0 # 0으로 라벨링
  worst_all.drop_duplicates(subset=['content'], inplace=True)
  worst_all = worst_all.dropna()
  worst_all= worst_all[:5000]


  ## 특수문자 제거 함수활용
  # 특수문자 제거함으로써 모델 입력 데이터의 품질을 높이기
  best['content'] = best['content'].apply(clean_review_text)

  # 2. 부정 리뷰 데이터프레임 정제
  worst_all['content'] = worst_all['content'].apply(clean_review_text)

  # 3. 라벨 (score) 재설정 (만약 평점 외의 다른 라벨링이 필요하다면)
  # 긍정 라벨은 1, 부정 라벨은 0으로 통일 (이진 분류 기준)
  best['label'] = 1
  worst_all['label'] = 0

  best = best.drop(columns=['score'])
  worst_all = worst_all.drop(columns=['score'])

  # 데이터 정규화
  best['content'] = best['content'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
  worst_all['content'] = worst_all['content'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")

  # 데이터 묶기
  df= pd.concat([best,worst_all])
  df.columns=['content','label']

  df.isnull().sum() # 결측치 확인

  # 각 라벨값이 유일한지 확인
  print(df['content'].nunique(), df['label'].nunique()) # 천건이 중복됨

  df= df.drop_duplicates(subset=['content'])
  df= df.reset_index(drop=True)

  return df

df_yo_p= Preprocessing(df_yo)
df_yo_p

df_bm_p= Preprocessing(df_bm)
df_bm_p

df_cp_p= Preprocessing(df_cp)
df_cp_p

## 배달의민족, 요기요, 쿠팡이츠 리뷰 데이터 합치기
data= pd.concat([df_bm_p,df_cp_p,df_yo_p])
data

##### 실행xxxxx ==> 임베딩 모델에서 불용어 자동으로 제거됨
# 불용어 제거
## df에서 불용어 제거할거 모아서 제거하기
#### 여기서부터

# import re

# # 이 문장이 포함되어 있으면 삭제
# stop_txt= ['//','..,','.,','ㅋㅋㅋ',';;','ㅠㅜ','...','ㅠㅠ','+)','bb','~~~~~','ㄱㄱ','良い','ㅈ맛','ㅣ솟',
#            '뱁신임','응애','걍','슈잠','갓겜','니뽕','배민갓','제주도제주도제주도제주도제주도추가추가','이스 ㅓ커ㅓ커',
#            '레기레기쓰레기','광고 시','미영이','김서인','독일 기업','ㄹㅇ','trash !!','너도안녕~~','젓같갇ㅈ','ㅈ구려요',
#            '렉 오지네','렉 쩌네요','바보바보바보바보바보바보바보바보','응 아니야~~,','형 간다~','ㄱㄹㅎㄷ','게르만민족 goodbye~',
#            '드러움','흐흐흐헝헝헝','ㄱ ㅐ겉애요','수수료 ㅌㅌ 턴업','우우우우우웅우우우우우운우우우우우우우우우우우우우우우우우웅우우우우우우우',
#            '니얼굴11  에드라떼 당첨되게 해주세염!','ㅋㅋㅋㅋㅋㅋ','ㅋㅋ','니얼굴','미X','ㄱ  ㅅ ㅂ    ㅆ   ㄲ ',
#            '졸라','갑질']

# # 불용어 리스트를 하나의 정규식 패턴으로 만듭니다.
# # '|'는 OR 연산을 의미하며, 're.escape'는 특수 문자를 처리합니다.
# pattern = '|'.join(map(re.escape, stop_txt)) # 단어1|단어2...식으로 문자열로 저장됨

# # 불용어가 포함되는 행 지우기
# # ~는 '아니다(NOT)'의 의미로, contains() 결과가 True인 행을 제거합니다.
# data = df[~df['content'].str.contains(pattern, case=False, na=False)] # case=False : 대소문자 구분x , na=False : 결측값 있는행 false로 처리

# data= pd.DataFrame(data)
# data = data.reset_index(drop=True)
# data # 41725

data.to_csv('data.csv',index=False)

"""## 2. 분류모델"""

pip install datasets

pip install langchain-community langchain-core

######### 여기서부터 실행하기, 이미 정제된 데이터를 저장해서 불러온후 LLM모델 돌리기
# 가상의 리뷰 데이터셋 생성
import pandas as pd
from datasets import Dataset

data= pd.read_csv('data.csv') # 약 3만건

if data.isnull().sum().any():
  data= data.dropna()

# 데이터셋을 Hugging Face의 Dataset 형식으로 변환
dataset = Dataset.from_pandas(data)

# # 학습용과 평가용으로 데이터셋 분할
dataset = dataset.train_test_split(test_size=0.2) # 0.3

data.head()

# KLUE/RoBERTa-base
# monologg/koelectra-base-v3-discriminator
# beomi/kcbert-base
# snunlp/KR-SBERT-V40K
# j-min/koelectra-base-v2-finetuned-sentiment
# KBLab/emotional-classification

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "KLUE/RoBERTa-base" ### 모델 바꿔보기

# 모델에 맞는 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained(model_name) # 문장을 단어, 형태소 같은 작은 단위(토큰)로 쪼개고 이를 숫자로 변환

# 감성 분류를 위한 사전 학습된 KoBERT 모델 불러오기
# num_labels=2는 긍정/부정 2가지 클래스를 예측한다는 의미
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # 텍스트를 생성하는 데 특화된 모델 아키텍처를 불러오기


'''
모델 돌려보고 정확도가 낮으면 이 토큰나이저로 수정해서 다시 돌려보기
배달어플의 리뷰 토큰나이저
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("monologg/kobert")
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
'''

## 위 코드로 숫자로 이미 변환된는데 토큰화할 이유는???
def tokenize_function(examples):
    # padding='max_length': 모든 문장의 길이를 통일
    # truncation=True: 최대 길이를 초과하는 문장을 자름
    return tokenizer(examples['content'], padding="max_length", truncation=True)

# 데이터셋의 각 문장을 토큰화
tokenized_datasets = dataset.map(tokenize_function, batched=True) # true: map함수가 여러샘을 배치형태로 묶어 한꺼번에 처리-> 속도향상

# 학습 및 평가 데이터셋 정의
# 데이터 셔플
train_dataset = tokenized_datasets["train"].shuffle(seed=42)
eval_dataset = tokenized_datasets["test"].shuffle(seed=42)

tokenized_datasets

train_dataset

eval_dataset

print("Train dataset unique labels:", set(train_dataset['label']))
print("Eval dataset unique labels:", set(eval_dataset['label']))

# UpstageAI의 API 키를 사용하여 Hugging Face 모델을 학습시키는 코드를 직접 작성하는 것은 불가능
# pipeline : 예측
# TrainingArguments : 모델학습

import torch
import numpy as np
import torch.nn as nn
from sklearn.metrics import accuracy_score
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback

def compute_metrics(p):
  preds = np.argmax(p.predictions, axis=1)
  acc = accuracy_score(p.label_ids, preds)
  return {'accuracy': acc}

# 학습에 사용할 하이퍼파라미터 설정
training_args = TrainingArguments(
    output_dir="./results",
    report_to="none",
    num_train_epochs=2, # 에포크 10으로 설정하였더니 정확도가 떨어짐, 에포크2로하니 정확도가 95%까지 올라감
    per_device_train_batch_size=8, # 4,8,16 으로 설정해보기
    per_device_eval_batch_size=8, # 4,8,16 으로 설정해보기
    learning_rate=3e-5,  # 0.00003, 기본값인 5e-5(0.00005)
    eval_strategy="epoch",
    save_strategy="epoch",  # 저장: 에포크마다
    logging_dir="./logs",

    # 조기 종료를 위한 설정 (가장 중요)
    load_best_model_at_end=True,      # 학습 종료 후 최적 모델 가중치를 로드합니다.
    metric_for_best_model="eval_loss", # Validation Loss 기준으로 최적 모델을 선택합니다.
    greater_is_better=False          # Loss는 낮을수록 좋으므로 False로 설정합니다.
)

# Validation Loss가 3번 연속 증가하면 학습을 멈춥니다.
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=2 # 3으로 했더니 정확도가 떨어짐
)

# 현재 0.999 긍정 오류가 발생했으므로, 부정 클래스(0)의 가중치를 높입니다.
# 긍정(LABEL_1): 1.0, 부정(LABEL_0): 1.5 로 설정하여 부정 오분류 시 패널티를 1.5배 부여
CLASS_WEIGHTS = torch.tensor([1.5, 1.0], dtype=torch.float32)


class CustomWeightedTrainer(Trainer):
    # 'num_items_in_batch'와 같은 불필요한 인수를 제거하고 표준 인수로만 남깁니다.
    def compute_loss(self, model, inputs, return_outputs=False,  **kwargs):

        # (나머지 손실 계산 로직은 그대로 유지)
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')

        # 3. Weighted CrossEntropyLoss 사용
        # self.model.config.num_labels는 모델이 몇 개의 클래스를 예측하는지에 대한 정보입니다.
        loss_fct = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(logits.device))

        # Loss 계산
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))

        return (loss, outputs) if return_outputs else loss


# Trainer 인스턴스 생성
trainer = CustomWeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # tokenized_datasets["train"] 대신 train_dataset 사용
    eval_dataset=eval_dataset,    # tokenized_datasets["test"] 대신 eval_dataset 사용
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

# 2만5천건 , 에포크 3 : 정확도는 미세하게 올라가지만 loss도 올라가짐
# 4만건, 에포크 2  : loss가 0.69, 정확도 0.53
# 3만건, 에포크2  : loss가 0.47 , 정확도 0.88

# 3만건, 에포크 5, test0.2 => 1시간걸림, 아래결과
# Epoch	Training Loss	Validation Loss	Accuracy
# 1	0.726700	2.275503	0.496000
# 2	0.766600	1.716054	0.504000
# 3	0.731800	2.115923	0.504000
# 4	0.673800	0.806315	0.504000
# 5	0.681800	0.708318	0.49600

## 정확도나 다른결과값이 좋지 않으면 허깅페이스에서 배달리뷰 토큰나이저 사용해보기

# 데이터 3만건, 에포크 6, learning_rate=3e-5, per_device_train_batch_size=8, test_size=0.2
# Epoch	Training Loss	Validation Loss	Accuracy
# 1	0.210200	0.199280	0.954217
# 2	0.121000	0.212766	0.955422
# 3	0.102900	0.252020	0.953356
# 4	0.078000	0.241302	0.954905
# 5	0.047100	0.274276	0.956454
# 6	0.016600	0.315993	0.954905  ==> 과적합발생

# 1시간 5분걸림 -> 데이터 3만건
# 에포크 0.87 -> 48분
# 학습 시작!
trainer.train()

# 긍정1, 부정0으로 학습함
### Training Loss가 nolog에 대한 문제잡기 --> 데이터의 갯수가 부족하다 -> 데이터양 늘리기
from transformers import pipeline
trainer.save_model("./my_emotional_classifier")

# 파이프라인(pipeline)을 사용하여 저장된 모델을 로드
# 텍스트 분류에 맞는 파이프라인 설정
classifier = pipeline("text-classification", model="./my_emotional_classifier", tokenizer="./my_emotional_classifier")

# 새로운 리뷰 입력
new_review1 = "이 카페 분위기 정말 좋고 커피도 맛있네요!"
new_review2 = "직원들이 너무 불친절해서 다시는 안 갈 거예요."
new_review3= '맛이 없어요'

# 예측 수행
result1 = classifier(new_review1)
result2 = classifier(new_review2)
result3 = classifier(new_review3)

print(f"리뷰: '{new_review1}' -> 예측: {result1}")
print(f"리뷰: '{new_review2}' -> 예측: {result2}")
print(f"리뷰: '{new_review3}' -> 예측: {result3}")

# 출력 결과:
# 모델이 'LABEL_0' 또는 'LABEL_1'과 같은 결과와 함께 신뢰도(score)를 반환합니다.
# 'LABEL_0'은 부정, 'LABEL_1'은 긍정을 의미합니다.

# Device set to use cuda:0
# 리뷰: '이 카페 분위기 정말 좋고 커피도 맛있네요!' -> 예측: [{'label': 'LABEL_1', 'score': 0.9920687675476074}]
# 리뷰: '직원들이 너무 불친절해서 다시는 안 갈 거예요.' -> 예측: [{'label': 'LABEL_0', 'score': 0.9843050837516785}]


# Device set to use cuda:0
# 리뷰: '이 카페 분위기 정말 좋고 커피도 맛있네요!' -> 예측: [{'label': 'LABEL_1', 'score': 0.999843955039978}]
# 리뷰: '직원들이 너무 불친절해서 다시는 안 갈 거예요.' -> 예측: [{'label': 'LABEL_0', 'score': 0.9993472695350647}]
# 리뷰: '맛이 없어요' -> 예측: [{'label': 'LABEL_1', 'score': 0.9998345375061035}]

### 이미저장됨 , 실행x
# 모델 저장
# import os

# # 1. 저장할 로컬 디렉토리 경로 설정
# save_directory = "./fine_tuned_koberta_model"

# # 3. 모델과 토크나이저 저장
# # 모델의 학습된 가중치와 구성 파일 (config.json) 저장
# model.save_pretrained(save_directory)

# # 토크나이저 관련 파일 (vocab.txt, special_tokens_map.json 등) 저장
# tokenizer.save_pretrained(save_directory)

# 모델 로드
# from transformers import AutoModelForSequenceClassification, AutoTokenizer

# # 저장된 로컬 디렉토리 경로
# load_directory = "./my_emotional_classifier"

# # 1. 모델 로드 (학습된 가중치 로드)
# loaded_model = AutoModelForSequenceClassification.from_pretrained(load_directory)

# # 2. 토크나이저 로드
# loaded_tokenizer = AutoTokenizer.from_pretrained(load_directory)

# # 3. 모델 사용 준비 (추론 전에 필수)
# loaded_model.eval()

# print("모델과 토크나이저 로드 완료. 이제 예측에 사용할 수 있습니다.")

"""### 웹페이지 제작
### https://ngrok.com/

### ngrok 터미널 삭제하기 : https://2pandi.tistory.com/7
"""

pip install streamlit pyngrok -q

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import pipeline
# 
# # 저장된 모델 경로 (Colab 환경에서 my_emotional_classifier 폴더가 있어야 합니다)
# MODEL_PATH = "ju03/Chatbot_Emotion-classification"   # "./my_emotional_classifier"
# 
# # 모델 로드 (앱이 로드될 때 한 번만 실행)
# @st.cache_resource
# def load_model():
#     # 텍스트 분류 파이프라인 사용
#     try:
#         classifier = pipeline(
#             "text-classification",
#             model=MODEL_PATH,
#             tokenizer=MODEL_PATH
#         )
#         return classifier
#     except Exception as e:
#         st.error(f"모델 로드 실패! 폴더가 있는지 확인하세요: {e}")
#         return None
# 
# classifier = load_model()
# 
# st.title('배달 앱 리뷰 감성 분류 봇 🤖')
# st.write('파인튜닝된 KLUE/RoBERTa 모델로 리뷰를 긍정/부정 분류합니다.')
# 
# # 텍스트 영역 생성
# review_text = st.text_area("리뷰를 여기에 입력하세요:", height=150)
# 
# if st.button('분류하기'):
#     if not classifier:
#         st.warning("모델이 로드되지 않아 분류를 진행할 수 없습니다.")
#     elif review_text.strip() == "":
#         st.warning("분류할 리뷰 텍스트를 입력해주세요.")
#     else:
#         # 진행률 표시줄
#         with st.spinner('리뷰 분석 중...'):
# 
#             # 예측 수행
#             result = classifier(review_text)[0]
# 
#         label = result['label']
#         score = result['score']
# 
#         # 결과 매핑
#         sentiment = '긍정 👍' if label == 'LABEL_1' else '부정 👎'
# 
#         st.success('✅ 분류 완료!')
#         st.metric(label="분류 결과", value=sentiment)
#         st.info(f"신뢰도: {score*100:.2f}%")

pip install pyngrok

# https://ngrok.com
# your_ngrok_authtoken_here 부분을 복사한 인증 토큰으로 대체하세요.
from pyngrok import ngrok

# ngrok 인증
NGROK_TOKEN = # your Authtoken
ngrok.set_auth_token(NGROK_TOKEN)

print("ngrok 인증 완료.")

pip install streamlit

import subprocess
import time
import streamlit as st

# Streamlit 앱을 백그라운드에서 실행합니다.
# `app.py`를 실행하고, Streamlit의 기본 포트 8501을 사용합니다.
p = subprocess.Popen(["streamlit", "run", "app.py", "--server.port", "8501"])

try:
    public_url = ngrok.connect(8501)
    print("웹 페이지 접속 주소:")
    print(f"{public_url}")

except Exception as e:
    print(f"오류 발생: {e}")
finally:
    pass