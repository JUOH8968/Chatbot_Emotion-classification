# -*- coding: utf-8 -*-
"""ë°°ë‹¬ì–´í”Œ ë¦¬ë·°ë°ì´í„°ë¡œ Chatbot ë§Œë“¤ê¸°.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GbxwsIxubMgEX9qW6pTCzg4Pb0ZVv7Ib

#### ë¶ˆìš©ì–´ ì‚¬ì „ : https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a

# LLM ë§Œë“¤ê¸°

### 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
"""

pip install google_play_scraper

### ë°°ë‹¬ì˜ ë¯¼ì¡± + ìš”ê¸°ìš” + ì¿ íŒ¡ì´ì¸  ë°ì´í„°ë¥¼ í•©ì¹œ 3ë§Œê±´ ë°ì´í„°ë¡œ ëª¨ë¸ëŒë¦¬ê¸°
### ëª¨ë¸ì˜ ë²”ìš©ì„±(ì¼ë°˜í™”ì„±ëŠ¥)ì¦ê°€, í¸í–¥ê°ì†Œ, í…ìŠ¤íŠ¸ì˜ ë‹¤ì–‘ì„± ì¦ê°€
## ë¶€ì •ë¦¬ë·°ë¥¼ ê¸ì •ë¦¬ë·°ë¡œ ì˜ëª»ì˜ˆì¸¡í•¨ -> ë¼ë²¨ìˆ˜ì •í•˜ê¸°, í™œì„±í™”í•¨ìˆ˜ì¸ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì‚¬ìš©, class_weightë¡œ ì†ì‹¤í•¨ìˆ˜ ê°€ì¤‘ì¹˜ë¶€ì—¬ , í•™ìŠµë°ì´í„°ì¦ê°•ê¸°ë²• ì‚¬ìš©í•˜ê¸°

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

# ë°°ë‹¬ì–´í”Œ ìˆœìœ„
# 1. ë°°ë‹¬ì˜ë¯¼ì¡±
# 2. ì¿ íŒ¡ì´ì¸ 
# 3. ìš”ê¸°ìš”

url = "com.sampleapp"
# com.coupang.mobile.eats-> ì¿ íŒ¡ì´ì¸ 
# com.fineapp.yogiyo --> ìš”ê¸°ìš”
# com.sampleapp --> ë°°ë‹¬ì˜ë¯¼ì¡±

# ì•± ì •ë³´ í¬ë¡¤ë§
app = app(url,lang = "ko", #ì–¸ì–´ í•œêµ­ì–´
                country = "kr") #ë‚˜ë¼ í•œêµ­ìœ¼ë¡œ ì„¤ì •

# ì•± ë¦¬ë·° í¬ë¡¤ë§
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5ì´ˆ ëŒ€ê¸°ì‹œê°„
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , ê´€ë ¨ì„± ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬
                    filter_score_with = None # None means All score
                    )
# ë°ì´í„°í”„ë ˆì„ ë³€í™˜
df_bm= pd.DataFrame(review)
df_bm.head(2) # 138278

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

# ë°°ë‹¬ì–´í”Œ ìˆœìœ„
# 1. ë°°ë‹¬ì˜ë¯¼ì¡±
# 2. ì¿ íŒ¡ì´ì¸ 
# 3. ìš”ê¸°ìš”

url = "com.coupang.mobile.eats"
# com.coupang.mobile.eats-> ì¿ íŒ¡ì´ì¸ 
# com.fineapp.yogiyo --> ìš”ê¸°ìš”
# com.sampleapp --> ë°°ë‹¬ì˜ë¯¼ì¡±

# ì•± ì •ë³´ í¬ë¡¤ë§
app = app(url,lang = "ko", #ì–¸ì–´ í•œêµ­ì–´
                country = "kr") #ë‚˜ë¼ í•œêµ­ìœ¼ë¡œ ì„¤ì •

# ì•± ë¦¬ë·° í¬ë¡¤ë§
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5ì´ˆ ëŒ€ê¸°ì‹œê°„
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , ê´€ë ¨ì„± ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬
                    filter_score_with = None # None means All score
                    )
# ë°ì´í„°í”„ë ˆì„ ë³€í™˜
df_cp= pd.DataFrame(review)
df_cp.head(2) # 138278

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

# ë°°ë‹¬ì–´í”Œ ìˆœìœ„
# 1. ë°°ë‹¬ì˜ë¯¼ì¡±
# 2. ì¿ íŒ¡ì´ì¸ 
# 3. ìš”ê¸°ìš”

url = "com.fineapp.yogiyo"
# com.coupang.mobile.eats-> ì¿ íŒ¡ì´ì¸ 
# com.fineapp.yogiyo --> ìš”ê¸°ìš”
# com.sampleapp --> ë°°ë‹¬ì˜ë¯¼ì¡±

# ì•± ì •ë³´ í¬ë¡¤ë§
app = app(url,lang = "ko", #ì–¸ì–´ í•œêµ­ì–´
                country = "kr") #ë‚˜ë¼ í•œêµ­ìœ¼ë¡œ ì„¤ì •

# ì•± ë¦¬ë·° í¬ë¡¤ë§
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5ì´ˆ ëŒ€ê¸°ì‹œê°„
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , ê´€ë ¨ì„± ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬
                    filter_score_with = None # None means All score
                    )
# ë°ì´í„°í”„ë ˆì„ ë³€í™˜
df_yo= pd.DataFrame(review)
df_yo.head(2) # 138278

"""### 2. ì „ì²˜ë¦¬- ë°°ë‹¬ì˜ë¯¼ì¡±"""

best_bm= df_bm[['content','score']][df_bm['score']==5] # score ê°’ì´ 1ì ì¸ ë¦¬ë·°ë§Œ ì¶”ì¶œ
best_bm['score']=1 # 1ë¡œ ë¼ë²¨ë§
best_bm.drop_duplicates(subset=['content'], inplace=True)
best_bm = best_bm.dropna()
best_bm= best_bm[:5000]
best_bm

### ë¦¬ë·°1ì ê³¼ 2ì ì„ 0ìœ¼ë¡œ ë¼ë²¨ë§ í•´ë³´ê¸°
worst_all_bm= df_bm[['content','score']] [(df_bm['score']==1) | (df_bm['score']==2)]
worst_all_bm['score']=0 # 0ìœ¼ë¡œ ë¼ë²¨ë§
worst_all_bm.drop_duplicates(subset=['content'], inplace=True)
worst_all_bm = worst_all_bm.dropna()
worst_all_bm= worst_all_bm[:5000]
worst_all_bm

# # ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µê°’ ì œê±°
# best.drop_duplicates(subset=['content'], inplace=True)
# worst_all.drop_duplicates(subset=['content'], inplace=True)

# best = best.dropna()
# print(len(best)) # 67949

# worst_all = worst_all.dropna()
# print(len(worst_all)) # 18663

### íŠ¹ìˆ˜ë¬¸ì ì œê±° í•¨ìˆ˜ì •ì˜
import re
def clean_review_text(text):
    # ì…ë ¥ í…ìŠ¤íŠ¸ê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
    if text is None:
        return ""

    # 4. í—ˆìš©í•  ë¬¸ì ì •ì˜
    # í•œê¸€, ì˜ì–´ ëŒ€ì†Œë¬¸ì, ìˆ«ì, ê·¸ë¦¬ê³  ê°ì„± í‘œí˜„ì— ì¤‘ìš”í•œ êµ¬ë‘ì (.,?!), ê³µë°±ë§Œ í—ˆìš©í•©ë‹ˆë‹¤.
    # [^ ] ì•ˆì— ì •ì˜ëœ ë¬¸ìì—´ ì™¸ì˜ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,?!]', ' ', text)

    # 5. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  ì–‘ìª½ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()

    return text

## íŠ¹ìˆ˜ë¬¸ì ì œê±° í•¨ìˆ˜í™œìš©
# íŠ¹ìˆ˜ë¬¸ì ì œê±°í•¨ìœ¼ë¡œì¨ ëª¨ë¸ ì…ë ¥ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ê¸°
best_bm['content'] = best_bm['content'].apply(clean_review_text)

# 2. ë¶€ì • ë¦¬ë·° ë°ì´í„°í”„ë ˆì„ ì •ì œ
worst_all_bm['content'] = worst_all_bm['content'].apply(clean_review_text)

# 3. ë¼ë²¨ (score) ì¬ì„¤ì • (ë§Œì•½ í‰ì  ì™¸ì˜ ë‹¤ë¥¸ ë¼ë²¨ë§ì´ í•„ìš”í•˜ë‹¤ë©´)
# ê¸ì • ë¼ë²¨ì€ 1, ë¶€ì • ë¼ë²¨ì€ 0ìœ¼ë¡œ í†µì¼ (ì´ì§„ ë¶„ë¥˜ ê¸°ì¤€)
best_bm['label'] = 1
worst_all_bm['label'] = 0

best_bm = best_bm.drop(columns=['score'])
worst_all_bm = worst_all_bm.drop(columns=['score'])

# ë°ì´í„° ì •ê·œí™”
best_bm['content'] = best_bm['content'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
worst_all_bm['content'] = worst_all_bm['content'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")

# ë°ì´í„° ë¬¶ê¸°
df_bm= pd.concat([best_bm,worst_all_bm])
df_bm.columns=['content','label']
df_bm

df_bm.isnull().sum() # ê²°ì¸¡ì¹˜ í™•ì¸

# ê° ë¼ë²¨ê°’ì´ ìœ ì¼í•œì§€ í™•ì¸
print(df_bm['content'].nunique(), df_bm['label'].nunique()) # ì²œê±´ì´ ì¤‘ë³µë¨
df_bm= df_bm.drop_duplicates(subset=['content'])

"""### 2-2. ì „ì²˜ë¦¬ - í•¨ìˆ˜ë¡œ ë¬¶ê¸°"""

### ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def Preprocessing(data):
  best= data[['content','score']][data['score']==5] # score ê°’ì´ 1ì ì¸ ë¦¬ë·°ë§Œ ì¶”ì¶œ
  best['score']=1 # 1ë¡œ ë¼ë²¨ë§
  best.drop_duplicates(subset=['content'], inplace=True)
  best = best.dropna()
  best= best[:5000]

  ### ë¦¬ë·°1ì ê³¼ 2ì ì„ 0ìœ¼ë¡œ ë¼ë²¨ë§ í•´ë³´ê¸°
  worst_all= data[['content','score']] [(data['score']==1) | (data['score']==2)]
  worst_all['score']=0 # 0ìœ¼ë¡œ ë¼ë²¨ë§
  worst_all.drop_duplicates(subset=['content'], inplace=True)
  worst_all = worst_all.dropna()
  worst_all= worst_all[:5000]


  ## íŠ¹ìˆ˜ë¬¸ì ì œê±° í•¨ìˆ˜í™œìš©
  # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•¨ìœ¼ë¡œì¨ ëª¨ë¸ ì…ë ¥ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ê¸°
  best['content'] = best['content'].apply(clean_review_text)

  # 2. ë¶€ì • ë¦¬ë·° ë°ì´í„°í”„ë ˆì„ ì •ì œ
  worst_all['content'] = worst_all['content'].apply(clean_review_text)

  # 3. ë¼ë²¨ (score) ì¬ì„¤ì • (ë§Œì•½ í‰ì  ì™¸ì˜ ë‹¤ë¥¸ ë¼ë²¨ë§ì´ í•„ìš”í•˜ë‹¤ë©´)
  # ê¸ì • ë¼ë²¨ì€ 1, ë¶€ì • ë¼ë²¨ì€ 0ìœ¼ë¡œ í†µì¼ (ì´ì§„ ë¶„ë¥˜ ê¸°ì¤€)
  best['label'] = 1
  worst_all['label'] = 0

  best = best.drop(columns=['score'])
  worst_all = worst_all.drop(columns=['score'])

  # ë°ì´í„° ì •ê·œí™”
  best['content'] = best['content'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
  worst_all['content'] = worst_all['content'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")

  # ë°ì´í„° ë¬¶ê¸°
  df= pd.concat([best,worst_all])
  df.columns=['content','label']

  df.isnull().sum() # ê²°ì¸¡ì¹˜ í™•ì¸

  # ê° ë¼ë²¨ê°’ì´ ìœ ì¼í•œì§€ í™•ì¸
  print(df['content'].nunique(), df['label'].nunique()) # ì²œê±´ì´ ì¤‘ë³µë¨

  df= df.drop_duplicates(subset=['content'])
  df= df.reset_index(drop=True)

  return df

df_yo_p= Preprocessing(df_yo)
df_yo_p

df_bm_p= Preprocessing(df_bm)
df_bm_p

df_cp_p= Preprocessing(df_cp)
df_cp_p

## ë°°ë‹¬ì˜ë¯¼ì¡±, ìš”ê¸°ìš”, ì¿ íŒ¡ì´ì¸  ë¦¬ë·° ë°ì´í„° í•©ì¹˜ê¸°
data= pd.concat([df_bm_p,df_cp_p,df_yo_p])
data

##### ì‹¤í–‰xxxxx ==> ì„ë² ë”© ëª¨ë¸ì—ì„œ ë¶ˆìš©ì–´ ìë™ìœ¼ë¡œ ì œê±°ë¨
# ë¶ˆìš©ì–´ ì œê±°
## dfì—ì„œ ë¶ˆìš©ì–´ ì œê±°í• ê±° ëª¨ì•„ì„œ ì œê±°í•˜ê¸°
#### ì—¬ê¸°ì„œë¶€í„°

# import re

# # ì´ ë¬¸ì¥ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì‚­ì œ
# stop_txt= ['//','..,','.,','ã…‹ã…‹ã…‹',';;','ã… ã…œ','...','ã… ã… ','+)','bb','~~~~~','ã„±ã„±','è‰¯ã„','ã…ˆë§›','ã…£ì†Ÿ',
#            'ë±ì‹ ì„','ì‘ì• ','ê±','ìŠˆì ','ê°“ê²œ','ë‹ˆë½•','ë°°ë¯¼ê°“','ì œì£¼ë„ì œì£¼ë„ì œì£¼ë„ì œì£¼ë„ì œì£¼ë„ì¶”ê°€ì¶”ê°€','ì´ìŠ¤ ã…“ì»¤ã…“ì»¤',
#            'ë ˆê¸°ë ˆê¸°ì“°ë ˆê¸°','ê´‘ê³  ì‹œ','ë¯¸ì˜ì´','ê¹€ì„œì¸','ë…ì¼ ê¸°ì—…','ã„¹ã…‡','trash !!','ë„ˆë„ì•ˆë…•~~','ì “ê°™ê°‡ã…ˆ','ã…ˆêµ¬ë ¤ìš”',
#            'ë ‰ ì˜¤ì§€ë„¤','ë ‰ ì©Œë„¤ìš”','ë°”ë³´ë°”ë³´ë°”ë³´ë°”ë³´ë°”ë³´ë°”ë³´ë°”ë³´ë°”ë³´','ì‘ ì•„ë‹ˆì•¼~~,','í˜• ê°„ë‹¤~','ã„±ã„¹ã…ã„·','ê²Œë¥´ë§Œë¯¼ì¡± goodbye~',
#            'ë“œëŸ¬ì›€','íííí—í—í—','ã„± ã…ê²‰ì• ìš”','ìˆ˜ìˆ˜ë£Œ ã…Œã…Œ í„´ì—…','ìš°ìš°ìš°ìš°ìš°ì›…ìš°ìš°ìš°ìš°ìš°ìš´ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ìš°ì›…ìš°ìš°ìš°ìš°ìš°ìš°ìš°',
#            'ë‹ˆì–¼êµ´11  ì—ë“œë¼ë–¼ ë‹¹ì²¨ë˜ê²Œ í•´ì£¼ì„¸ì—¼!','ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹','ã…‹ã…‹','ë‹ˆì–¼êµ´','ë¯¸X','ã„±  ã…… ã…‚    ã…†   ã„² ',
#            'ì¡¸ë¼','ê°‘ì§ˆ']

# # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
# # '|'ëŠ” OR ì—°ì‚°ì„ ì˜ë¯¸í•˜ë©°, 're.escape'ëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
# pattern = '|'.join(map(re.escape, stop_txt)) # ë‹¨ì–´1|ë‹¨ì–´2...ì‹ìœ¼ë¡œ ë¬¸ìì—´ë¡œ ì €ì¥ë¨

# # ë¶ˆìš©ì–´ê°€ í¬í•¨ë˜ëŠ” í–‰ ì§€ìš°ê¸°
# # ~ëŠ” 'ì•„ë‹ˆë‹¤(NOT)'ì˜ ì˜ë¯¸ë¡œ, contains() ê²°ê³¼ê°€ Trueì¸ í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.
# data = df[~df['content'].str.contains(pattern, case=False, na=False)] # case=False : ëŒ€ì†Œë¬¸ì êµ¬ë¶„x , na=False : ê²°ì¸¡ê°’ ìˆëŠ”í–‰ falseë¡œ ì²˜ë¦¬

# data= pd.DataFrame(data)
# data = data.reset_index(drop=True)
# data # 41725

data.to_csv('data.csv',index=False)

"""## 2. ë¶„ë¥˜ëª¨ë¸"""

pip install datasets

pip install langchain-community langchain-core

######### ì—¬ê¸°ì„œë¶€í„° ì‹¤í–‰í•˜ê¸°, ì´ë¯¸ ì •ì œëœ ë°ì´í„°ë¥¼ ì €ì¥í•´ì„œ ë¶ˆëŸ¬ì˜¨í›„ LLMëª¨ë¸ ëŒë¦¬ê¸°
# ê°€ìƒì˜ ë¦¬ë·° ë°ì´í„°ì…‹ ìƒì„±
import pandas as pd
from datasets import Dataset

data= pd.read_csv('data.csv') # ì•½ 3ë§Œê±´

if data.isnull().sum().any():
  data= data.dropna()

# ë°ì´í„°ì…‹ì„ Hugging Faceì˜ Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
dataset = Dataset.from_pandas(data)

# # í•™ìŠµìš©ê³¼ í‰ê°€ìš©ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„í• 
dataset = dataset.train_test_split(test_size=0.2) # 0.3

data.head()

# KLUE/RoBERTa-base
# monologg/koelectra-base-v3-discriminator
# beomi/kcbert-base
# snunlp/KR-SBERT-V40K
# j-min/koelectra-base-v2-finetuned-sentiment
# KBLab/emotional-classification

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "KLUE/RoBERTa-base" ### ëª¨ë¸ ë°”ê¿”ë³´ê¸°

# ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_name) # ë¬¸ì¥ì„ ë‹¨ì–´, í˜•íƒœì†Œ ê°™ì€ ì‘ì€ ë‹¨ìœ„(í† í°)ë¡œ ìª¼ê°œê³  ì´ë¥¼ ìˆ«ìë¡œ ë³€í™˜

# ê°ì„± ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‚¬ì „ í•™ìŠµëœ KoBERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# num_labels=2ëŠ” ê¸ì •/ë¶€ì • 2ê°€ì§€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œë‹¤ëŠ” ì˜ë¯¸
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° íŠ¹í™”ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°


'''
ëª¨ë¸ ëŒë ¤ë³´ê³  ì •í™•ë„ê°€ ë‚®ìœ¼ë©´ ì´ í† í°ë‚˜ì´ì €ë¡œ ìˆ˜ì •í•´ì„œ ë‹¤ì‹œ ëŒë ¤ë³´ê¸°
ë°°ë‹¬ì–´í”Œì˜ ë¦¬ë·° í† í°ë‚˜ì´ì €
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("monologg/kobert")
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
'''

## ìœ„ ì½”ë“œë¡œ ìˆ«ìë¡œ ì´ë¯¸ ë³€í™˜ëœëŠ”ë° í† í°í™”í•  ì´ìœ ëŠ”???
def tokenize_function(examples):
    # padding='max_length': ëª¨ë“  ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ í†µì¼
    # truncation=True: ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ë¬¸ì¥ì„ ìë¦„
    return tokenizer(examples['content'], padding="max_length", truncation=True)

# ë°ì´í„°ì…‹ì˜ ê° ë¬¸ì¥ì„ í† í°í™”
tokenized_datasets = dataset.map(tokenize_function, batched=True) # true: mapí•¨ìˆ˜ê°€ ì—¬ëŸ¬ìƒ˜ì„ ë°°ì¹˜í˜•íƒœë¡œ ë¬¶ì–´ í•œêº¼ë²ˆì— ì²˜ë¦¬-> ì†ë„í–¥ìƒ

# í•™ìŠµ ë° í‰ê°€ ë°ì´í„°ì…‹ ì •ì˜
# ë°ì´í„° ì…”í”Œ
train_dataset = tokenized_datasets["train"].shuffle(seed=42)
eval_dataset = tokenized_datasets["test"].shuffle(seed=42)

tokenized_datasets

train_dataset

eval_dataset

print("Train dataset unique labels:", set(train_dataset['label']))
print("Eval dataset unique labels:", set(eval_dataset['label']))

# UpstageAIì˜ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ Hugging Face ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥
# pipeline : ì˜ˆì¸¡
# TrainingArguments : ëª¨ë¸í•™ìŠµ

import torch
import numpy as np
import torch.nn as nn
from sklearn.metrics import accuracy_score
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback

def compute_metrics(p):
  preds = np.argmax(p.predictions, axis=1)
  acc = accuracy_score(p.label_ids, preds)
  return {'accuracy': acc}

# í•™ìŠµì— ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results",
    report_to="none",
    num_train_epochs=2, # ì—í¬í¬ 10ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ë”ë‹ˆ ì •í™•ë„ê°€ ë–¨ì–´ì§, ì—í¬í¬2ë¡œí•˜ë‹ˆ ì •í™•ë„ê°€ 95%ê¹Œì§€ ì˜¬ë¼ê°
    per_device_train_batch_size=8, # 4,8,16 ìœ¼ë¡œ ì„¤ì •í•´ë³´ê¸°
    per_device_eval_batch_size=8, # 4,8,16 ìœ¼ë¡œ ì„¤ì •í•´ë³´ê¸°
    learning_rate=3e-5,  # 0.00003, ê¸°ë³¸ê°’ì¸ 5e-5(0.00005)
    eval_strategy="epoch",
    save_strategy="epoch",  # ì €ì¥: ì—í¬í¬ë§ˆë‹¤
    logging_dir="./logs",

    # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ì„¤ì • (ê°€ì¥ ì¤‘ìš”)
    load_best_model_at_end=True,      # í•™ìŠµ ì¢…ë£Œ í›„ ìµœì  ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    metric_for_best_model="eval_loss", # Validation Loss ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
    greater_is_better=False          # LossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ Falseë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
)

# Validation Lossê°€ 3ë²ˆ ì—°ì† ì¦ê°€í•˜ë©´ í•™ìŠµì„ ë©ˆì¶¥ë‹ˆë‹¤.
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=2 # 3ìœ¼ë¡œ í–ˆë”ë‹ˆ ì •í™•ë„ê°€ ë–¨ì–´ì§
)

# í˜„ì¬ 0.999 ê¸ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìœ¼ë¯€ë¡œ, ë¶€ì • í´ë˜ìŠ¤(0)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì…ë‹ˆë‹¤.
# ê¸ì •(LABEL_1): 1.0, ë¶€ì •(LABEL_0): 1.5 ë¡œ ì„¤ì •í•˜ì—¬ ë¶€ì • ì˜¤ë¶„ë¥˜ ì‹œ íŒ¨ë„í‹°ë¥¼ 1.5ë°° ë¶€ì—¬
CLASS_WEIGHTS = torch.tensor([1.5, 1.0], dtype=torch.float32)


class CustomWeightedTrainer(Trainer):
    # 'num_items_in_batch'ì™€ ê°™ì€ ë¶ˆí•„ìš”í•œ ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê³  í‘œì¤€ ì¸ìˆ˜ë¡œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    def compute_loss(self, model, inputs, return_outputs=False,  **kwargs):

        # (ë‚˜ë¨¸ì§€ ì†ì‹¤ ê³„ì‚° ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')

        # 3. Weighted CrossEntropyLoss ì‚¬ìš©
        # self.model.config.num_labelsëŠ” ëª¨ë¸ì´ ëª‡ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤.
        loss_fct = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(logits.device))

        # Loss ê³„ì‚°
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))

        return (loss, outputs) if return_outputs else loss


# Trainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
trainer = CustomWeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # tokenized_datasets["train"] ëŒ€ì‹  train_dataset ì‚¬ìš©
    eval_dataset=eval_dataset,    # tokenized_datasets["test"] ëŒ€ì‹  eval_dataset ì‚¬ìš©
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

# 2ë§Œ5ì²œê±´ , ì—í¬í¬ 3 : ì •í™•ë„ëŠ” ë¯¸ì„¸í•˜ê²Œ ì˜¬ë¼ê°€ì§€ë§Œ lossë„ ì˜¬ë¼ê°€ì§
# 4ë§Œê±´, ì—í¬í¬ 2  : lossê°€ 0.69, ì •í™•ë„ 0.53
# 3ë§Œê±´, ì—í¬í¬2  : lossê°€ 0.47 , ì •í™•ë„ 0.88

# 3ë§Œê±´, ì—í¬í¬ 5, test0.2 => 1ì‹œê°„ê±¸ë¦¼, ì•„ë˜ê²°ê³¼
# Epoch	Training Loss	Validation Loss	Accuracy
# 1	0.726700	2.275503	0.496000
# 2	0.766600	1.716054	0.504000
# 3	0.731800	2.115923	0.504000
# 4	0.673800	0.806315	0.504000
# 5	0.681800	0.708318	0.49600

## ì •í™•ë„ë‚˜ ë‹¤ë¥¸ê²°ê³¼ê°’ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë°°ë‹¬ë¦¬ë·° í† í°ë‚˜ì´ì € ì‚¬ìš©í•´ë³´ê¸°

# ë°ì´í„° 3ë§Œê±´, ì—í¬í¬ 6, learning_rate=3e-5, per_device_train_batch_size=8, test_size=0.2
# Epoch	Training Loss	Validation Loss	Accuracy
# 1	0.210200	0.199280	0.954217
# 2	0.121000	0.212766	0.955422
# 3	0.102900	0.252020	0.953356
# 4	0.078000	0.241302	0.954905
# 5	0.047100	0.274276	0.956454
# 6	0.016600	0.315993	0.954905  ==> ê³¼ì í•©ë°œìƒ

# 1ì‹œê°„ 5ë¶„ê±¸ë¦¼ -> ë°ì´í„° 3ë§Œê±´
# ì—í¬í¬ 0.87 -> 48ë¶„
# í•™ìŠµ ì‹œì‘!
trainer.train()

# ê¸ì •1, ë¶€ì •0ìœ¼ë¡œ í•™ìŠµí•¨
### Training Lossê°€ nologì— ëŒ€í•œ ë¬¸ì œì¡ê¸° --> ë°ì´í„°ì˜ ê°¯ìˆ˜ê°€ ë¶€ì¡±í•˜ë‹¤ -> ë°ì´í„°ì–‘ ëŠ˜ë¦¬ê¸°
from transformers import pipeline
trainer.save_model("./my_emotional_classifier")

# íŒŒì´í”„ë¼ì¸(pipeline)ì„ ì‚¬ìš©í•˜ì—¬ ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œ
# í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ë§ëŠ” íŒŒì´í”„ë¼ì¸ ì„¤ì •
classifier = pipeline("text-classification", model="./my_emotional_classifier", tokenizer="./my_emotional_classifier")

# ìƒˆë¡œìš´ ë¦¬ë·° ì…ë ¥
new_review1 = "ì´ ì¹´í˜ ë¶„ìœ„ê¸° ì •ë§ ì¢‹ê³  ì»¤í”¼ë„ ë§›ìˆë„¤ìš”!"
new_review2 = "ì§ì›ë“¤ì´ ë„ˆë¬´ ë¶ˆì¹œì ˆí•´ì„œ ë‹¤ì‹œëŠ” ì•ˆ ê°ˆ ê±°ì˜ˆìš”."
new_review3= 'ë§›ì´ ì—†ì–´ìš”'

# ì˜ˆì¸¡ ìˆ˜í–‰
result1 = classifier(new_review1)
result2 = classifier(new_review2)
result3 = classifier(new_review3)

print(f"ë¦¬ë·°: '{new_review1}' -> ì˜ˆì¸¡: {result1}")
print(f"ë¦¬ë·°: '{new_review2}' -> ì˜ˆì¸¡: {result2}")
print(f"ë¦¬ë·°: '{new_review3}' -> ì˜ˆì¸¡: {result3}")

# ì¶œë ¥ ê²°ê³¼:
# ëª¨ë¸ì´ 'LABEL_0' ë˜ëŠ” 'LABEL_1'ê³¼ ê°™ì€ ê²°ê³¼ì™€ í•¨ê»˜ ì‹ ë¢°ë„(score)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# 'LABEL_0'ì€ ë¶€ì •, 'LABEL_1'ì€ ê¸ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

# Device set to use cuda:0
# ë¦¬ë·°: 'ì´ ì¹´í˜ ë¶„ìœ„ê¸° ì •ë§ ì¢‹ê³  ì»¤í”¼ë„ ë§›ìˆë„¤ìš”!' -> ì˜ˆì¸¡: [{'label': 'LABEL_1', 'score': 0.9920687675476074}]
# ë¦¬ë·°: 'ì§ì›ë“¤ì´ ë„ˆë¬´ ë¶ˆì¹œì ˆí•´ì„œ ë‹¤ì‹œëŠ” ì•ˆ ê°ˆ ê±°ì˜ˆìš”.' -> ì˜ˆì¸¡: [{'label': 'LABEL_0', 'score': 0.9843050837516785}]


# Device set to use cuda:0
# ë¦¬ë·°: 'ì´ ì¹´í˜ ë¶„ìœ„ê¸° ì •ë§ ì¢‹ê³  ì»¤í”¼ë„ ë§›ìˆë„¤ìš”!' -> ì˜ˆì¸¡: [{'label': 'LABEL_1', 'score': 0.999843955039978}]
# ë¦¬ë·°: 'ì§ì›ë“¤ì´ ë„ˆë¬´ ë¶ˆì¹œì ˆí•´ì„œ ë‹¤ì‹œëŠ” ì•ˆ ê°ˆ ê±°ì˜ˆìš”.' -> ì˜ˆì¸¡: [{'label': 'LABEL_0', 'score': 0.9993472695350647}]
# ë¦¬ë·°: 'ë§›ì´ ì—†ì–´ìš”' -> ì˜ˆì¸¡: [{'label': 'LABEL_1', 'score': 0.9998345375061035}]

### ì´ë¯¸ì €ì¥ë¨ , ì‹¤í–‰x
# ëª¨ë¸ ì €ì¥
# import os

# # 1. ì €ì¥í•  ë¡œì»¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
# save_directory = "./fine_tuned_koberta_model"

# # 3. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
# # ëª¨ë¸ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ì™€ êµ¬ì„± íŒŒì¼ (config.json) ì €ì¥
# model.save_pretrained(save_directory)

# # í† í¬ë‚˜ì´ì € ê´€ë ¨ íŒŒì¼ (vocab.txt, special_tokens_map.json ë“±) ì €ì¥
# tokenizer.save_pretrained(save_directory)

# ëª¨ë¸ ë¡œë“œ
# from transformers import AutoModelForSequenceClassification, AutoTokenizer

# # ì €ì¥ëœ ë¡œì»¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ
# load_directory = "./my_emotional_classifier"

# # 1. ëª¨ë¸ ë¡œë“œ (í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ)
# loaded_model = AutoModelForSequenceClassification.from_pretrained(load_directory)

# # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
# loaded_tokenizer = AutoTokenizer.from_pretrained(load_directory)

# # 3. ëª¨ë¸ ì‚¬ìš© ì¤€ë¹„ (ì¶”ë¡  ì „ì— í•„ìˆ˜)
# loaded_model.eval()

# print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ. ì´ì œ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

"""### ì›¹í˜ì´ì§€ ì œì‘
### https://ngrok.com/

### ngrok í„°ë¯¸ë„ ì‚­ì œí•˜ê¸° : https://2pandi.tistory.com/7
"""

pip install streamlit pyngrok -q

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import pipeline
# 
# # ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ (Colab í™˜ê²½ì—ì„œ my_emotional_classifier í´ë”ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
# MODEL_PATH = "ju03/Chatbot_Emotion-classification"   # "./my_emotional_classifier"
# 
# # ëª¨ë¸ ë¡œë“œ (ì•±ì´ ë¡œë“œë  ë•Œ í•œ ë²ˆë§Œ ì‹¤í–‰)
# @st.cache_resource
# def load_model():
#     # í…ìŠ¤íŠ¸ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
#     try:
#         classifier = pipeline(
#             "text-classification",
#             model=MODEL_PATH,
#             tokenizer=MODEL_PATH
#         )
#         return classifier
#     except Exception as e:
#         st.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨! í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: {e}")
#         return None
# 
# classifier = load_model()
# 
# st.title('ë°°ë‹¬ ì•± ë¦¬ë·° ê°ì„± ë¶„ë¥˜ ë´‡ ğŸ¤–')
# st.write('íŒŒì¸íŠœë‹ëœ KLUE/RoBERTa ëª¨ë¸ë¡œ ë¦¬ë·°ë¥¼ ê¸ì •/ë¶€ì • ë¶„ë¥˜í•©ë‹ˆë‹¤.')
# 
# # í…ìŠ¤íŠ¸ ì˜ì—­ ìƒì„±
# review_text = st.text_area("ë¦¬ë·°ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”:", height=150)
# 
# if st.button('ë¶„ë¥˜í•˜ê¸°'):
#     if not classifier:
#         st.warning("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë¶„ë¥˜ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     elif review_text.strip() == "":
#         st.warning("ë¶„ë¥˜í•  ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
#     else:
#         # ì§„í–‰ë¥  í‘œì‹œì¤„
#         with st.spinner('ë¦¬ë·° ë¶„ì„ ì¤‘...'):
# 
#             # ì˜ˆì¸¡ ìˆ˜í–‰
#             result = classifier(review_text)[0]
# 
#         label = result['label']
#         score = result['score']
# 
#         # ê²°ê³¼ ë§¤í•‘
#         sentiment = 'ê¸ì • ğŸ‘' if label == 'LABEL_1' else 'ë¶€ì • ğŸ‘'
# 
#         st.success('âœ… ë¶„ë¥˜ ì™„ë£Œ!')
#         st.metric(label="ë¶„ë¥˜ ê²°ê³¼", value=sentiment)
#         st.info(f"ì‹ ë¢°ë„: {score*100:.2f}%")

pip install pyngrok

# https://ngrok.com
# your_ngrok_authtoken_here ë¶€ë¶„ì„ ë³µì‚¬í•œ ì¸ì¦ í† í°ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
from pyngrok import ngrok

# ngrok ì¸ì¦
NGROK_TOKEN = # your Authtoken
ngrok.set_auth_token(NGROK_TOKEN)

print("ngrok ì¸ì¦ ì™„ë£Œ.")

pip install streamlit

import subprocess
import time
import streamlit as st

# Streamlit ì•±ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
# `app.py`ë¥¼ ì‹¤í–‰í•˜ê³ , Streamlitì˜ ê¸°ë³¸ í¬íŠ¸ 8501ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
p = subprocess.Popen(["streamlit", "run", "app.py", "--server.port", "8501"])

try:
    public_url = ngrok.connect(8501)
    print("ì›¹ í˜ì´ì§€ ì ‘ì† ì£¼ì†Œ:")
    print(f"{public_url}")

except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
finally:
    pass